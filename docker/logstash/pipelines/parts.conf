input {
  jdbc {
    jdbc_driver_library => "/usr/share/logstash/logstash-core/lib/jars/mysql-connector-java.jar"
    jdbc_driver_class => "com.mysql.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://host.docker.internal:3306/${MYSQL_DATABASE}?allowPublicKeyRetrieval=true&serverTimezone=Europe/Moscow&useSSL=false"
    jdbc_default_timezone => "Europe/Moscow"
    jdbc_user => "${MYSQL_USER}"
    jdbc_password => "${MYSQL_PASSWORD}"
    jdbc_paging_enabled => "true"
    jdbc_page_size => "50000"
    tracking_column => "updated_at"
    tracking_column_type => "timestamp"
    use_column_value => "true"
    schedule => "*/60 * * * * *"
    statement => "SELECT pr.*, pt.name, pt.model, pt.color, pt.size, pt.weight, pt.description FROM products pr JOIN parts pt ON pr.id = pt.product_id WHERE pr.updated_at > :sql_last_value ORDER BY pr.updated_at"
    last_run_metadata_path => "/tmp/parts_logstash_jdbc_last_run"
    clean_run => "true"
  }
}
filter {
  prune {
    whitelist_names => ["^type$", "^id$", "^price$", "^published$", "^name$", "^model$", "^color$", "^size$", "^weight$", "^description$", "^created_at$", "^updated_at$"]
  }
}
output {
  elasticsearch {
    index => "parts"
    document_type => "parts"
    hosts => "elasticsearch:9200"
    user => "elastic"
    password => "elastic"
    document_id => "%{id}"
    retry_initial_interval => 10
  }
  stdout {
    codec => "rubydebug"
  }
}